{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bJLQoimyVyQ8"
   },
   "source": [
    "# Semantic Search using Open-source Transformers and Faiss\n",
    "\n",
    "Ref: [How to Build a Semantic Search Engine With Transformers and Faiss](https://towardsdatascience.com/how-to-build-a-semantic-search-engine-with-transformers-and-faiss-dcbea307a0e8)\n",
    "\n",
    "**GPU can help to run this demo faster**\n",
    "\n",
    "For all the required packages:\n",
    "`!pip install -r requirements.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fU2i4vlCVyRc"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "import faiss\n",
    "\n",
    "#import s3fs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "    \n",
    "from vector_engine.utils import vector_search, id2details   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vbnscDwgVyRW"
   },
   "source": [
    "## Raw Data\n",
    "\n",
    "Misinformation, disinformation and fake news papers processed and stored in csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 143
    },
    "id": "HJXljSbYVyRn",
    "outputId": "1c180fbc-42a4-441a-da47-14f5cc21d826"
   },
   "outputs": [],
   "source": [
    "#df = pd.read_csv(\"s3://vector-search-blog/misinformation_papers.csv\")\n",
    "df = pd.read_csv(\"../data/misinformation_papers.csv\")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MljadlGpVyRs",
    "outputId": "8f2f5205-772f-4c6b-f445-5dd32350d45e"
   },
   "outputs": [],
   "source": [
    "print(f\"Misinformation, disinformation and fake news papers: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding using Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VyRG1wZLVyRw"
   },
   "source": [
    "[SentenceTransformers](https://www.sbert.net/)\n",
    "\n",
    "The [Sentence Transformers library](https://github.com/UKPLab/sentence-transformers) offers pretrained transformers that produce SOTA sentence embeddings. Checkout this [spreadsheet](https://docs.google.com/spreadsheets/d/14QplCdTCDwEmTqrn1LH4yrbKvdogK4oQvYO1K1aPR5M/) with all the available models.\n",
    "\n",
    "The `distilbert-base-nli-stsb-mean-tokens` model has the best performance on Semantic Textual Similarity tasks among the DistilBERT versions. Moreover, comparing to BERT, it is smaller and faster.\n",
    "\n",
    "BTW, [Orion's semantic search engine](https://www.orion-search.org/) is using the same model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PjF6CrwUVyRx",
    "outputId": "db338335-b032-45f2-db21-8e3f53640b86"
   },
   "outputs": [],
   "source": [
    "# Instantiate the sentence-level DistilBERT\n",
    "model = SentenceTransformer('distilbert-base-nli-stsb-mean-tokens')\n",
    "# Check if GPU is available and use it\n",
    "if torch.cuda.is_available():\n",
    "    model = model.to(torch.device(\"cuda\"))\n",
    "print(model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 66,
     "referenced_widgets": [
      "7a7e927567024c578b33b15000d5e531",
      "da5011a6565e45e2b5d0b37634498648",
      "3e9c24c8488b431fb88a0d045d85b700",
      "7aaf8a423e7f48bbac24d6970ed4dfe9",
      "f9c22552944b4e2dafe1f98170665293",
      "15c5bbc768db41e4bc06c9405539091c",
      "1e1408cddd284bc4a4b5484be0561991",
      "7f1cd9dbb4b742ab8d78e073fb9b0f90"
     ]
    },
    "id": "Y_GS0_CWVyR1",
    "outputId": "4deb0814-1ce9-4ea8-8e50-72992e0ea303"
   },
   "outputs": [],
   "source": [
    "# Convert abstracts to vectors\n",
    "embeddings = model.encode(df.abstract.to_list(), show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gE7w-RJbVyR6",
    "outputId": "0451849a-88ef-4aee-be2d-e6ff173782f3"
   },
   "outputs": [],
   "source": [
    "print(f\"Shape of the vectorised abstract: {embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YGV4Je1EVyR_"
   },
   "source": [
    "## Vector similarity search with Faiss\n",
    "\n",
    "\n",
    "\n",
    "[Faiss](https://github.com/facebookresearch/faiss) is a library for efficient similarity search and clustering of dense vectors. It contains algorithms that search in sets of vectors of any size, even ones that do not fit in RAM. [ref: Faiss Paper](https://arxiv.org/abs/1702.08734)\n",
    "    \n",
    "Faiss is built around the `Index` object which contains, and sometimes preprocesses, the searchable vectors. Faiss has a large collection of [indexes](https://github.com/facebookresearch/faiss/wiki/Faiss-indexes). You can even create [composite indexes](https://github.com/facebookresearch/faiss/wiki/Faiss-indexes-(composite)). Faiss handles collections of vectors of a fixed dimensionality d, typically a few 10s to 100s.\n",
    "\n",
    "**Note**: Faiss uses only 32-bit floating point matrices. This means that you will have to change the data type of the input before building the index.\n",
    "\n",
    "\n",
    "Here, we will the `IndexFlatL2` index:\n",
    "- It's a simple index that performs a brute-force L2 distance search\n",
    "- It scales linearly. It will work fine with our data but you might want to try [faster indexes](https://github.com/facebookresearch/faiss/wiki/Faster-search) if you work will millions of vectors.\n",
    "\n",
    "To create an index with the `misinformation` abstract vectors, we will:\n",
    "1. Change the data type of the abstract vectors to float32.\n",
    "2. Build an index and pass it the dimension of the vectors it will operate on.\n",
    "3. Pass the index to IndexIDMap, an object that enables us to provide a custom list of IDs for the indexed vectors.\n",
    "4. Add the abstract vectors and their ID mapping to the index. In our case, we will map vectors to their paper IDs from MAG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Change data type\n",
    "embeddings_f32 = embeddings.astype(\"float32\")\n",
    "\n",
    "# Step 2: Instantiate the index\n",
    "index = faiss.IndexFlatL2(embeddings_f32.shape[1])\n",
    "\n",
    "# Step 3: Pass the index to IndexIDMap\n",
    "index = faiss.IndexIDMap(index)\n",
    "\n",
    "# Step 4: Add vectors and their IDs\n",
    "index.add_with_ids(embeddings_f32, df.id.values)\n",
    "\n",
    "print(f\"Number of vectors in the Faiss index: {index.ntotal}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yt1z-433VySE"
   },
   "source": [
    "### Searching the index\n",
    "The index we built will perform a k-nearest-neighbour search. We have to provide the number of neighbours to be returned. \n",
    "\n",
    "Let's query the index with an abstract from our dataset and retrieve the 10 most relevant documents. **The first one must be our query!**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 106
    },
    "id": "eEeJt7lYVySN",
    "outputId": "771571b3-0200-48b8-f2de-4e8cdbd5ec98"
   },
   "outputs": [],
   "source": [
    "# pick a paper abstract\n",
    "random_pick = 5415\n",
    "df.iloc[random_pick][\"abstract\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BSuRcH85VySQ",
    "outputId": "cec93a12-e79c-4f0a-fc15-45048a3469aa"
   },
   "outputs": [],
   "source": [
    "# Retrieve the 10 nearest neighbours\n",
    "D, I = index.search(np.array([embeddings[random_pick]]), k=10)\n",
    "print(f'L2 distance: {D.flatten().tolist()}\\n\\nMAG paper IDs: {I.flatten().tolist()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SiO1pa4oVySU",
    "outputId": "24c9d1ea-3951-4b4a-e436-c077f3528d7e"
   },
   "outputs": [],
   "source": [
    "# Fetch the paper titles based on their index\n",
    "id2details(df, I, 'original_title')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p29pEtGrWUMV",
    "outputId": "b6448614-0a6a-4673-c841-2bfb0340607e"
   },
   "outputs": [],
   "source": [
    "# Fetch the paper abstracts based on their index\n",
    "id2details(df, I, 'abstract')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gFKvRb4QY-DL"
   },
   "source": [
    "\n",
    "## Query\n",
    "\n",
    "So far, we've built a Faiss index using the misinformation abstract vectors we encoded with a sentence-DistilBERT model. That's helpful but in a real case scenario, we would have to work with unseen data. To query the index with an unseen query and retrieve its most relevant documents, we would have to do the following:\n",
    "\n",
    "1. Encode the query with the same sentence-DistilBERT model we used for the rest of the abstract vectors.\n",
    "2. Change its data type to float32.\n",
    "3. Search the index with the encoded query.\n",
    "\n",
    "Here, we will use the introduction of an article published on [HKS Misinformation Review](https://misinforeview.hks.harvard.edu/article/can-whatsapp-benefit-from-debunked-fact-checked-stories-to-reduce-misinformation/).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iDhftkrhX99T"
   },
   "outputs": [],
   "source": [
    "user_query = \"\"\"\n",
    "WhatsApp was alleged to have been widely used to spread misinformation and propaganda \n",
    "during the 2018 elections in Brazil and the 2019 elections in India. Due to the \n",
    "private encrypted nature of the messages on WhatsApp, it is hard to track the dissemination \n",
    "of misinformation at scale. In this work, using public WhatsApp data from Brazil and India, we \n",
    "observe that misinformation has been largely shared on WhatsApp public groups even after they \n",
    "were already fact-checked by popular fact-checking agencies. This represents a significant portion \n",
    "of misinformation spread in both Brazil and India in the groups analyzed. We posit that such \n",
    "misinformation content could be prevented if WhatsApp had a means to flag already fact-checked \n",
    "content. To this end, we propose an architecture that could be implemented by WhatsApp to counter \n",
    "such misinformation. Our proposal respects the current end-to-end encryption architecture on WhatsApp, \n",
    "thus protecting users’ privacy while providing an approach to detect the misinformation that benefits \n",
    "from fact-checking efforts.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6AFhbGnWZpWN",
    "outputId": "b8a02af0-2f0d-4740-984a-e804405b3e6a"
   },
   "outputs": [],
   "source": [
    "# For convenience, I've wrapped all steps in the vector_search function.\n",
    "# It takes four arguments: \n",
    "# A query, the sentence-level transformer, the Faiss index and the number of requested results\n",
    "D, I = vector_search([user_query], model, index, num_results=10)\n",
    "print(f'L2 distance: {D.flatten().tolist()}\\n\\nMAG paper IDs: {I.flatten().tolist()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tbanjBhBZtWZ",
    "outputId": "9a5d6d97-8983-4253-8095-b7d899e33ac8"
   },
   "outputs": [],
   "source": [
    "# Fetching the paper titles based on their index\n",
    "id2details(df, I, 'original_title')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rbxFKF-DZxg0",
    "outputId": "d78cdc03-41f6-469d-bf67-8f32001a7415"
   },
   "outputs": [],
   "source": [
    "# Define project base directory\n",
    "# Change the index from 1 to 0 if you run this on Google Colab\n",
    "project_dir = Path('notebooks').resolve().parents[1]\n",
    "print(project_dir)\n",
    "\n",
    "# Serialise index and store it as a pickle\n",
    "with open(f\"{project_dir}/models/faiss_index.pickle\", \"wb\") as h:\n",
    "    pickle.dump(faiss.serialize_index(index), h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AtSC6oDDjtMA"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "001-vector-search.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-11.m109",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-11:m109"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "269.275px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
