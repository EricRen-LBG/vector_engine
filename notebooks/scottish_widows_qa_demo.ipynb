{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a961303",
   "metadata": {},
   "source": [
    "# Question-Answering Demo using Scottish Widows Public Documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ccf247-931e-4434-9092-b94a498d5a49",
   "metadata": {},
   "source": [
    "## Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3403d93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import faiss\n",
    "\n",
    "\n",
    "pd.options.display.max_rows = 100\n",
    "pd.options.display.max_columns = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e8fa02-952d-43f0-bd11-789b9d3675ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import vertexai\n",
    "from vertexai.preview.language_models import TextGenerationModel, TextEmbeddingModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b408de-d98d-4c60-829b-bdd79c01fc4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = ! gcloud config get core/project\n",
    "PROJECT_ID = PROJECT_ID[0]\n",
    "\n",
    "REGION = \"europe-west2\"\n",
    "\n",
    "PROJECT_ID, REGION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce297e5-7864-4526-95ef-222760328583",
   "metadata": {},
   "outputs": [],
   "source": [
    "%env PROJECT_ID=$PROJECT_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d225003c-8712-47d0-8648-95eaecf7bc84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ba31ef8c",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "### Raw data\n",
    "\n",
    "Document Source: Based on Scottish Widows' literature library search:\n",
    "https://adviser.scottishwidows.co.uk/literature-library.html\n",
    "\n",
    "Specifically for this demo, the *guides* are selected:\n",
    "https://adviser.scottishwidows.co.uk/literature-library.html?n=1000&filter=swe:literaturelibrary/contenttype/guides\n",
    "\n",
    "The pdf files are scrapped and save in local parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d6080c",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_guides_file = \"../data/scottish_widows_all_guides.pq\"\n",
    "\n",
    "guides_df = pd.read_parquet(all_guides_file)\n",
    "\n",
    "guides_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa6f0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "guides_df.groupby([\"title\"])[[\"page_number\"]].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a2ae797-ba3e-4a4e-89ea-0352419e4f1d",
   "metadata": {},
   "source": [
    "### Pre-processing\n",
    "#### Remove the blank pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b88f69-0846-4abb-ab70-3d4d263976b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(guides_df.shape)\n",
    "\n",
    "guides_df = guides_df.loc[guides_df[\"page_text\"]!=\"\"]\n",
    "\n",
    "print(guides_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc0cfe9",
   "metadata": {},
   "source": [
    "## Embedding using Google's `TextEmbedding` Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e7d6d3b",
   "metadata": {},
   "source": [
    "**Approach 1: Using the natural pages as chunks**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a417f6b-af69-4c73-812c-b2dd916fb302",
   "metadata": {},
   "outputs": [],
   "source": [
    "guides_df[\"page_text\"].loc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac3c3a2b-8d13-41b7-8826-1671bfe24ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TextEmbeddingModel.from_pretrained(\"textembedding-gecko@001\")\n",
    "\n",
    "# return a list of vertexai.language_models._language_models.TextEmbedding\n",
    "#embeddings = model.get_embeddings( [guides_df[\"page_text\"].loc[0]] )\n",
    "embeddings = model.get_embeddings(guides_df[\"page_text\"].loc[0:4]) # maximum 5 instance per embedding!\n",
    "\n",
    "len(embeddings), type(embeddings[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c292af38-db6f-42c9-975b-81956f967ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for embedding in embeddings:\n",
    "    vector = np.array(embedding.values)\n",
    "    print(vector.shape)\n",
    "    print(vector[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71fbe513-b719-43bf-8e35-2e0c46bdca7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series([embedding.values for embedding in embeddings], name=\"embedding\").to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b454d3-6f57-4458-b6ec-efdf060ec14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding_google(se, chunk_size=5):\n",
    "    \"\"\"Using Google's pretrained TextEmbeddingModel to vetorise the text series \n",
    "       Input:\n",
    "           se: Series of string\n",
    "           chunk_size: number of text items send to Google API.\n",
    "                       By default, GCP can process maximum 5 itmes in one go, \n",
    "                       so the chunk_size should be less than 5\n",
    "        Return: Numpy array with shape (m, n), where m is the number of text \n",
    "                and n the vector length (768 for Google model)\n",
    "    \"\"\"\n",
    "    \n",
    "    model = TextEmbeddingModel.from_pretrained(\"textembedding-gecko@001\")\n",
    "    \n",
    "    # generator use to iterate the series over smaller series with chunk_size rows) \n",
    "    small_se_gen = (se.iloc[i:i+chunk_size] for i in range(0, len(se), chunk_size))\n",
    "    small_se_embeddings = [model.get_embeddings(small_se) for small_se in small_se_gen]\n",
    "\n",
    "    eb_list = [\n",
    "        np.array(embedding.values, dtype=\"float32\") \n",
    "        for embeddings in small_se_embeddings \n",
    "        for embedding in embeddings\n",
    "        ]\n",
    "    return np.vstack(eb_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f63a24-8c55-47db-872d-b01289bd9874",
   "metadata": {},
   "source": [
    "**To test the embdding function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48dba024-8ddd-410b-8b2b-718fb0735a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# one text item each time for the 11 items embedding\n",
    "v1 = get_embedding_google(guides_df[\"page_text\"].iloc[0:11], 1)\n",
    "v1.shape, v1[0][0:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "342d9c41-bede-4e0d-ba4a-00753b13d181",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the default chunk size of 5 \n",
    "v2 = get_embedding_google(guides_df[\"page_text\"].iloc[0:11])\n",
    "v2.shape,  v2[0][0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e7e6fda-8770-4bbc-9e78-1b60c1cb5409",
   "metadata": {},
   "source": [
    "**Note: when more than one piece of text items are send for embedding, the model returns slightly different embeddings vector. But they are very similar when using the dot product to compare!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d4ee44-15c5-4a7a-bd1a-19843d931f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "[np.dot(v_1, v_2) for v_1, v_2 in zip(np.rollaxis(v1, 0), np.rollaxis(v2,0))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a44468-7288-4a2d-a119-3c9f76644264",
   "metadata": {},
   "source": [
    "### Embedding the whole set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6542bed-2ee1-4e4d-9afb-c2a8897ecc40",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%timeit -n 1 -r 1 # how long does is take? about 1 min for 1000 rows\n",
    "#get_embedding_google(guides_df[\"page_text\"].iloc[0:100])\n",
    "\n",
    "guides_embedded_df = pd.DataFrame(\n",
    "    get_embedding_google(guides_df[\"page_text\"]), index=guides_df.index\n",
    ")\n",
    "\n",
    "guides_embedded_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070cdf41-40eb-4f88-a6bd-f6de82539f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "guides_df.shape, guides_embedded_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7811158-bc6d-45e1-9882-d01883aeba8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3110ba-35a3-440d-9c05-958353c712a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "guides_embedded_file = \"../data/scottish_widows_all_guides_embedded_v3.pq\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee4a59e-e020-45c1-83a5-499ecb5e572c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# guides_embedded_df.to_parquet(guides_embedded_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d80520-cfdb-4e0d-a0ec-e039f0327ad2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "81bc5c9f-12e8-48be-ad9d-5837e9d9f79a",
   "metadata": {},
   "source": [
    "## Vector DB using Faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209a2b35-8d58-413c-a634-ecdc0bf6ce33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# guides_embedded_df = pd.read_parquet(guides_embedded_file)\n",
    "\n",
    "guides_embedded_df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d686d21a-e1fc-4c35-9e3e-4dba6cc114f5",
   "metadata": {},
   "source": [
    "### Create unique ID to link embedded vectors with original page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e663a86-0bb4-4eb2-859d-ecc7a05d497c",
   "metadata": {},
   "outputs": [],
   "source": [
    "guides_df = guides_df.reset_index().rename(columns={\"index\": \"id\"})\n",
    "\n",
    "guides_df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb6c18bf-6c9b-41d1-9fd0-2b3be3652408",
   "metadata": {},
   "source": [
    "### Build the vector DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4ff3bd-df80-4c38-9468-9967eea34af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate the index\n",
    "vector_length = guides_embedded_df.shape[1]\n",
    "\n",
    "index = faiss.IndexFlatL2(vector_length)\n",
    "\n",
    "# Pass the index to IndexIDMap and add vectors with IDs\n",
    "indexed = faiss.IndexIDMap(index)\n",
    "indexed.add_with_ids(guides_embedded_df, guides_df.id.values)\n",
    "\n",
    "print(f\"Number of vectors in the Faiss index: {indexed.ntotal}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6972f569",
   "metadata": {},
   "source": [
    "## Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "669ed572-ebb1-427f-b367-35725ae43129",
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomly pick up a page and search. The results should include itself as the closest match\n",
    "pick_page = 15 \n",
    "\n",
    "em = guides_embedded_df.iloc[pick_page:pick_page+1, :]\n",
    "distances, ids = indexed.search(em, k=3)\n",
    "print(f'L2 distance: {distances[0]}\\nIDs: {ids[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f05378-30c4-4533-842c-0643de9c9fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "guides_df[ guides_df.id.isin(ids[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "997f00c7-89ca-4a56-8d3a-c2286f88f59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector_search_google(query:str, index, num_results:int=3):\n",
    "    \"\"\"\n",
    "    Encoding the query using Google's text embedding model and search the closetest matches from vector DB\n",
    "    quert: the text to be embedded\n",
    "    index: faiss.swigfaiss_avx2.IndexFlatL2 as vector DB\n",
    "    num_results: number of matches to ruturn\n",
    "    \n",
    "    Returns:\n",
    "        distances: distances between results and query as float or numpy array.\n",
    "        ids: IDs of the maches as array.\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    model = TextEmbeddingModel.from_pretrained(\"textembedding-gecko@001\")\n",
    "    \n",
    "    query_vector = np.array(model.get_embeddings([query])[0].values, dtype=\"float32\").reshape(1, -1)\n",
    "\n",
    "    distances, ids = index.search(query_vector, k=num_results)\n",
    "    \n",
    "    return distances, ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebcd88de-34de-4a19-a68c-bd9c000ea607",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_query = \"\"\"How does the Discounted Gift & Income Trust work?\"\"\"\n",
    "\n",
    "ds, ids = vector_search_google(user_query, indexed, num_results=3)\n",
    "\n",
    "print(f'Euclidean distance: {ds[0]}\\nPage IDs: {ids[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92309fd-c125-4e2f-afac-13b14248172c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch the paper titles based on their index\n",
    "guides_df[ guides_df[\"id\"].isin(ids[0])]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6858937-a00b-411c-b9c2-448dac06ef43",
   "metadata": {},
   "source": [
    "## Answer the query based on the relevant pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b05ad27",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = guides_df[\"page_text\"][ guides_df[\"id\"].isin(ids[0])].iloc[0]\n",
    "question = \"\"\"How does the Discounted Gift & Income Trust work?\"\"\"\n",
    "\n",
    "template = f\"\"\"You are an expert having a conversation with a user.\n",
    "Given the following extracted parts of a long document and a question,\n",
    "create a final answer. \n",
    "{context}\n",
    "\n",
    "user: {question}\n",
    "expert:\n",
    "\"\"\"\n",
    "\n",
    "parameters = {\n",
    "    \"temperature\": 0.2,\n",
    "    \"max_output_tokens\": 256,   \n",
    "    \"top_p\": .8,                \n",
    "    \"top_k\": 40,                 \n",
    "}\n",
    "\n",
    "model = TextGenerationModel.from_pretrained(\"text-bison@001\")\n",
    "response = model.predict(template, **parameters)\n",
    "\n",
    "print(f\"Question: {question}\\n\")\n",
    "print(f\"Response from Model: \\n{response.text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3fb35d8-88aa-4a02-abc7-593fecf2abcb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "512e34c2-9cf2-42d1-96fe-e64d6c1fa7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_text_google(input_text, temperature: float=0.2) -> None:\n",
    "    parameters = {\n",
    "        \"temperature\": temperature,\n",
    "        \"max_output_tokens\": 256,   \n",
    "        \"top_p\": .8,                \n",
    "        \"top_k\": 40,                 \n",
    "    }\n",
    "\n",
    "    model = TextGenerationModel.from_pretrained(\"text-bison@001\")\n",
    "    response = model.predict(\n",
    "        input_text,\n",
    "        **parameters,\n",
    "    )\n",
    "    print(f\"Response from Model: \\n{response.text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c532afe-6db8-4663-8b3e-c5a4ee9cb57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"\"\"How does the Discounted Gift & Income Trust work?\"\"\"\n",
    "\n",
    "ds, ids = vector_search_google(question, indexed, num_results=3)\n",
    "\n",
    "context = guides_df[\"page_text\"][ guides_df[\"id\"].isin(ids[0])].iloc[0]\n",
    "\n",
    "#style = \"a concise way\"\n",
    "style = \"details\"\n",
    "\n",
    "text = f\"\"\"You are an expert having a conversation with a user.\n",
    "Given the following extracted parts of a long document and a question,\n",
    "create a final answer in {style}. \n",
    "{context}\n",
    "\n",
    "user: {question}\n",
    "expert:\n",
    "\"\"\"\n",
    "\n",
    "print(f\"Question: {question}\\n\")\n",
    "\n",
    "gen_text_google(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60dc5ddd-8f85-4436-86e0-e9ba58123533",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb7f6ea-26a7-48b9-8fc0-011fc6a3cb61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cba5e0a-ad49-4ae8-bff5-4cff8cd50e3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e151b763-30da-4c9f-95de-cb8428d4d88e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4966e799-c333-423b-b9c3-5322ff94ff61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1ab3a18d-6aca-479d-9b46-c60f21768e60",
   "metadata": {},
   "source": [
    "## Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386c7ac4-25c9-4c26-92b5-4ca17aed4fcc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4ce428-9e86-438f-ada7-f8f3e4637a17",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-11.m110",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-11:m110"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
