{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a961303",
   "metadata": {},
   "source": [
    "# Question-Answering with LangChain and GPT-3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba31ef8c",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "Document Source: the content of the web portal [FOSS](https://archive.ph/o/8NCVk/https://itsfoss.com/), which specializes in Open Source technologies, with a particular focus on Linux.\n",
    "\n",
    "A list of all the articles to process can be found from the site's [sitemap-posts.xml file](https://news.itsfoss.com/sitemap-posts.xml), which contains a list of links to all the articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3403d93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import xmltodict\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm.notebook import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d6080c",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get(\"https://news.itsfoss.com/sitemap-posts.xml\")\n",
    "xml = r.text\n",
    "rss = xmltodict.parse(xml)\n",
    "\n",
    "article_links = [entry[\"loc\"] for entry in rss[\"urlset\"][\"url\"]]\n",
    "\n",
    "print(f\"Total number of articles: {len(article_links)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa6f0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_content(url):\n",
    "    html = requests.get(url).text\n",
    "    soup = BeautifulSoup(html, features=\"html.parser\")\n",
    "    elements = [\n",
    "        soup.select_one(\".c-topper__headline\"),\n",
    "        soup.select_one(\".c-topper__standfirst\"),\n",
    "        soup.select_one(\".c-content\"),\n",
    "        ]\n",
    "    \n",
    "    text = \"\".join([element.get_text() for element in elements])\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9009e60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limited the list of articles to 10 for demo only\n",
    "articles = (\n",
    "    [{\"source\": url, \"content\": extract_content(url)}\n",
    "     for url in tqdm(article_links[0:10], desc=\"Extracting article content\")\n",
    "     ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6425b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "articles[0][\"source\"], articles[0][\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e497d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_df = pd.DataFrame(articles)\n",
    "articles_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc0cfe9",
   "metadata": {},
   "source": [
    "## Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e7d6d3b",
   "metadata": {},
   "source": [
    "### Splitting into chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e48c60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "rec_splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=150)\n",
    "\n",
    "web_docs, meta = [], []\n",
    "for article in tqdm(articles, desc=\"Splitting articles into chunks\"):\n",
    "    splits = rec_splitter.split_text(article[\"content\"])\n",
    "    web_docs.extend(splits)\n",
    "    meta.extend([{\"source\": article[\"source\"]}] * len(splits))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1258cbc3",
   "metadata": {},
   "source": [
    "### Embedding chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc9a830",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "# make sure the OPENAI_API_KEY environment variable has been set to be the OpenAI key \n",
    "#os.environ[\"OPENAI_API_KEY\"] = \"YOUR KEY\"\n",
    "article_store = FAISS.from_texts(\n",
    "    texts=web_docs, embedding=OpenAIEmbeddings(), metadatas=meta)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6972f569",
   "metadata": {},
   "source": [
    "## Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4314d18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key=\"chat_history\",\n",
    "    input_key=\"question\",\n",
    "    output_key=\"answer\",\n",
    "    return_messages=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c254e208",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "\n",
    "template = \"\"\"You are a chatbot having a conversation with a human.\n",
    "Given the following extracted parts of a long document and a question,\n",
    "create a final answer.\n",
    "{context}\n",
    "{chat_history}\n",
    "Human: {question}\n",
    "Chatbot:\"\"\"\n",
    "\n",
    "question_prompt = PromptTemplate(\n",
    "    input_variables=[\"chat_history\", \"question\", \"context\"], \n",
    "    template=template\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8d0569",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import OpenAI, PromptTemplate\n",
    "from langchain.chains import RetrievalQAWithSourcesChain\n",
    "\n",
    "article_chain = RetrievalQAWithSourcesChain.from_llm(\n",
    "    llm=OpenAI(temperature=0.0),\n",
    "    retriever=article_store.as_retriever(k=4),\n",
    "    memory=memory,\n",
    "    question_prompt=question_prompt,\n",
    "    )\n",
    "\n",
    "result = article_chain(\n",
    "    {\"question\": \"What is Skiff?\"},\n",
    "    return_only_outputs=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b4b9cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b05ad27",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-11.m109",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-11:m109"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
